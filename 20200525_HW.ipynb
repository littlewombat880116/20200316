{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  function Extract_NER():   Extract NER from receiving text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_NER(ls_textRow):\n",
    "    from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "    \n",
    "    # 載入 自訂詞典\n",
    "    User_Dict = {}\n",
    "    with open(\"KCC Data/Dict/KccDict2020.txt\",\"r\", encoding='utf-8-sig') as UDicts:\n",
    "        for udic in UDicts:\n",
    "            udWord = udic.strip().split(\" \")\n",
    "            if len(udWord) > 1:\n",
    "                User_Dict[udWord[0]] = udWord[1]\n",
    "            else:\n",
    "                User_Dict[udWord[0]] = 10    # 未給定權重者一律賦予預設值 10                \n",
    "    dictionary = construct_dictionary(User_Dict)\n",
    " \n",
    "    ws = WS(\"./data\")\n",
    "    pos = POS(\"./data\")\n",
    "    ner = NER(\"./data\")\n",
    "    \n",
    "    #  ls_segRow = ws(ls_textRow) ------------   without User Dictionary\n",
    "    ls_segRow = ws(ls_textRow, \n",
    "                   sentence_segmentation=True,\n",
    "                   segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\", \"、\"},\n",
    "                   coerce_dictionary = dictionary)\n",
    "    #print(\"ls_segRow = \", ls_segRow)\n",
    "    #print(\"-----------------------------\")\n",
    "    \n",
    "    del ws\n",
    "    del WS\n",
    "    gc.collect()\n",
    "    ls_posRow = pos(ls_segRow)\n",
    "    del pos\n",
    "    del POS\n",
    "    gc.collect()\n",
    "    ls_nerRow = ner(ls_segRow,ls_posRow)\n",
    "    del ner\n",
    "    del NER\n",
    "    gc.collect()\n",
    "    #print(\"ls_nerRow = \", ls_nerRow)\n",
    "    #print(\"-----------------------------\")\n",
    "    return ls_nerRow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data from csv or xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1', '2016/4/18',\n",
      "       '馬英九總統昨天上午視察衛武營藝術文化中心，文化部長洪孟啟、工程會副主委顏久榮及高雄市副市長吳宏謀等人也一同到場，聽取衛武營興建工程簡報、瞭解工程進度與執行情形，迄今已完成逾百分之九十五的工程進度，共計投入新臺幣一百零五億八千萬元經費，預計今年六月竣工、以明年開館營運為目標。馬英九總統指出，衛武營這個案子是從九十四年時編列預算核定施工，到正式完工的時間長達十三年，從原先預計的五年，一差就差了八年，其中就和故宮南院遇到相同問題，對於一個沒有工程單位的的部門要如何克服這種困難，值得大家共同檢討。馬總統致詞時表示，民國九十九年親自主持動土典禮，看著原從一片黃土，到如今生長出有如魟魚般悠遊於海浪的建築，實在令人驚豔，從無到有的過程中，包含多少團隊的付出及努力，雖然籌建過程非常艱辛，但在各界不斷的關注及投入下，今天看到新建工程已幾近完工，藉此特別肯定文化部在籌建過程中付出的辛勞，高雄市政府及各界的大力支持配合。總統並強調，只有文化才能讓一個城市變偉大，期許衛武營藝術文化中心未來不僅是臺灣藝術力量的南方集結地，也將與臺北國家兩廳院、臺中國家歌劇院攜手並進，共同打造台灣在國際藝文版圖的新地標。文化部長洪孟啟則表示，未來衛武營最大的考驗是需面臨營運壓力，因此文化部將以衛武營作為南部的文化中心，往上結合嘉義文化園區與故宮南院，以及台南地方藝文團體，往下則與高雄大東文化藝術中心、屏東演藝廳結合成文化廊帶，未來衛武營更將與國際接軌，文化部已邀請台灣旅德指揮家簡文彬擔任衛武國家藝術文化中心藝術總監，並積極接洽國外藝文團隊，希望國際型表演節目未來第一站就到高雄演出，讓衛武營成為台灣重要的文化指標場館。'],\n",
      "      dtype='object')\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#讀檔\n",
    "\n",
    "df_file = pd.read_csv(r\"/Users/wombat/Desktop/課程筆記/政治與資訊/NSYSU1082（莊老師）/Lab/KCC Data/Nwes/News500.csv\")\n",
    "print(df_file.columns)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "df_file.head(3)\n",
    "df_file.columns = ['index','file','Text']\n",
    "df_file = df_file.iloc[0:20,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_file = df_file.iloc[0:50,]\n",
    "#只跑0-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Main function:\n",
    "### 2.1  Pass text data to function \"Extract_NER\" by row\n",
    "### 2.2  Integrate every 10 NER return rows into one data frame  \n",
    "### 2.3  Write frame into small csv after droping duplicate, locating class, filtering short NER, sorting by class.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 欄位筆數 =  20\n",
      "------------------------------------------\n",
      "Row No =  0\n",
      "Row No =  1\n",
      "Row No =  2\n",
      "Row No =  3\n",
      "Row No =  4\n",
      "Row No =  5\n",
      "Row No =  6\n",
      "Row No =  7\n",
      "Row No =  8\n",
      "Row No =  9\n",
      "fileCunt No =  1\n",
      "Row No =  10\n",
      "Row No =  11\n",
      "Row No =  12\n",
      "Row No =  13\n",
      "Row No =  14\n",
      "Row No =  15\n",
      "Row No =  16\n",
      "Row No =  17\n",
      "Row No =  18\n",
      "Row No =  19\n",
      "fileCunt No =  2\n",
      "=====================================================================\n",
      "Totle NER Files =  2\n"
     ]
    }
   ],
   "source": [
    "#逐筆傳遞欄位 \"text\"\n",
    "df_text = df_file[\"Text\"]\n",
    "print(\"Text 欄位筆數 = \",len(df_text))\n",
    "print(\"------------------------------------------\")\n",
    "ls_class = list()\n",
    "ls_ner = list()\n",
    "fileCunt = 0\n",
    "rowCunt = 0\n",
    "for i in range(len(df_text)):\n",
    "# for i in range(6):\n",
    "    rowCunt += 1\n",
    "    ls_dfRow = [df_text[i]]\n",
    "    # print(\"ls_dfRow = \", ls_dfRow)\n",
    "    print(\"Row No = \",str(i))\n",
    "    entity_list = Extract_NER(ls_dfRow)\n",
    "    for j in range(len(entity_list[0])):\n",
    "        seg = entity_list[0].pop()\n",
    "        # print(\"seg = \",seg)\n",
    "        ls_class.append(seg[2])\n",
    "        ls_ner.append(seg[3])\n",
    "    #print(\"===========================================\")\n",
    "    #print(\"ls_class = \",ls_class)\n",
    "    #print(\"ls_ner = \",ls_ner)\n",
    "    # if i == 5 or rowCunt == 2:\n",
    "    if i == (len(df_text)-1) or rowCunt == 10:\n",
    "        fileCunt += 1\n",
    "        extract_df = pd.DataFrame({'Class':ls_class, 'NER':ls_ner})\n",
    "        deDup_df = extract_df.drop_duplicates().copy()\n",
    "        del extract_df\n",
    "        cate_df = deDup_df.loc[deDup_df['Class'].isin(['LOC','PERSON', 'ORG', 'LAW', 'EVENT','GPE','NORP'])].copy()\n",
    "        del deDup_df\n",
    "        filt_df = cate_df[cate_df['NER'].map(len) >= 2].copy()\n",
    "        del cate_df\n",
    "        NER_df = filt_df.sort_values(by=['Class']).copy()\n",
    "        del filt_df\n",
    "        #print(\"__________________________________________________________\")  \n",
    "        #print(NER_df)\n",
    "        print(\"fileCunt No = \",str(fileCunt))\n",
    "        # NER_df.to_csv(r\"News Data\\News NER Out\\\\NewsNER870_\" + str(fileCunt)+ \".csv\")\n",
    "        NER_df.to_csv(r\"ner out/NerTest_\" + str(fileCunt)+ \".csv\")\n",
    "        rowCunt = 0\n",
    "print(\"=====================================================================\")\n",
    "print(\"Totle NER Files = \",fileCunt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrate all csv files into single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EVENT</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPE</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAW</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NORP</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NER\n",
       "Class      \n",
       "EVENT     7\n",
       "GPE      58\n",
       "LAW       3\n",
       "LOC       9\n",
       "NORP     16\n",
       "ORG     153\n",
       "PERSON  122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# path = r'News Data\\News NER OUT'\n",
    "path = r'ner out'\n",
    "allFiles = glob.glob(\"{}/*.csv\".format(path))\n",
    "li = []\n",
    "for filename in allFiles:\n",
    "    df = pd.read_csv(filename, index_col=0, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "nerFrame = pd.concat(li, axis=0, ignore_index=True)\n",
    "print(len(nerFrame))\n",
    "nerFrame.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop duplicates, group by class and write into csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EVENT</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPE</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAW</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NORP</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NER\n",
       "Class      \n",
       "EVENT     6\n",
       "GPE      34\n",
       "LAW       2\n",
       "LOC       9\n",
       "NORP     10\n",
       "ORG     100\n",
       "PERSON   98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddFrame = nerFrame.drop_duplicates().copy()\n",
    "print(len(ddFrame))\n",
    "ddFrame.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddFrame.sort_values(by=['Class']).to_csv(r\"News Data\\News NER OUT\\\\NewsNER.csv\")\n",
    "ddFrame.sort_values(by=['Class']).to_csv(r\"ner out/NerTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
